{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **04. 결제 예측 모델링**\r\n",
    "--------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **목적**\r\n",
    "* ### 결제 행동을 예측하는 모델을 구축하고 평가한다.\r\n",
    "* ### 최적의 파라메터를 찾는 방법을 실습한다.\r\n",
    "<br>\r\n",
    "\r\n",
    "## **목차**\r\n",
    "* ###  문제 정의 및 가설\r\n",
    "* ###  분석 Frame 구성\r\n",
    "* ###  데이터 전처리\r\n",
    "* ###  데이터 분석\r\n",
    "* ###  리포트 작성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **04-1. 문제 정의 및 가설**\r\n",
    "--------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 여러 번 강조해도 지나침이 없을 정도로, 문제를 명확히 정의/설정하고 목표를 세우는 것은 매우 중요하다. 만약 목표가 틀리다면, 아무리 열심히 수고하더라도 큰 의미가 없기 때문이다.(물론 회사 입장에서 그렇다. 개인 입장에서는 경험치 획득에 도움이 될 수는 있다) 이번에는 마케팅팀에서 업무 요청이 왔다고 가정한다. 마케팅팀이 관심을 두는 것은 회원의 행동 패턴을 근거로, 결제로 전환할 유저를 모델을 통해 예측하는 것이 가능한지 여부이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **배경**\r\n",
    "### 배경 및 상황에 대해 조금더 자세히 살펴보자. 마케팅팀은 주로 예산을 집행하고 이에 따른 결과(성과)를 통해 예산을 효율화하는 것이 관심을 두고 있다. 이를 위해 흔히 알려진 전략이 STP이다. 즉 유저/소비자를 세분화해 그룹으로 나누고, 각 그룹에 맞는 개인화 메시지를 전달하거나 그룹의 우선순위에 따라 예산 및 접근 방식에 차등을 두는 방식을 주로 이용한다. 이번 사례에서는 결제자의 수를 증가시키기 위한 비즈니스 전략을 달성하기 위해 마케팅팀으로부터 결제자 예측 모델링 구축 관련한 업무 요청이 왔다고 가정하고 진행한다.\r\n",
    "\r\n",
    "* ### 결제자수의 증가에 대한 비즈니스 요구 발생 (마케팅팀 KPI)\r\n",
    "* ### 유저의 행동을 기반으로, 결제 가능성이 높은 유저를 선별하여 해당 유저를 위주로 타깃팅 (광고, 프로모션 등)\r\n",
    "* ### 데이터 분석가 뿐만 아니라, 여러 팀의 협업이 필요한 상황"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **목적**\r\n",
    "### 이번 사례의 경우 아래와 같이 데이터 분석가의 목적을 설정할 수 있다. 단, 지금 사례와 같이 여러 팀이 협업해서 프로젝트를 진행하는 경우라면 전체 프로젝트의 목적과 각 팀 혹은 파트 별로의 목적을 충분한 논의를 통해 설정하는 것이 필요하다.\r\n",
    "\r\n",
    "* ### 결제자로 전환할 가능성이 높은 유저를 판별하는 모델 구축\r\n",
    "* ### 예측 모델의 성능을 측정할 수 있는 지표를 정의"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\28.png \"28_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **가설**\r\n",
    "### 이번 사례의 경우 가설을 설정하기 위해 도메인 지식이 많이 요구된다. 도메인 지식과 전문가의 조언 등을 통해 결제와 관련 있는 행동 변수를 사전에 이해하고 리스트업하는 것이 유용하다. 만약 이러한 도움을 받기 어려운 상황이라면, 분석가의 상상력과 직감을 이용해 주요 변수들을 생각해놓고 탐색적 데이터 분석을 통해 Feature Selection을 하는 것이 필요하다.\r\n",
    "\r\n",
    "* ### 가설 1: Open, Edit/Save, Export와 같은 문서 이용행동 관련 변수가 모델 구축에 유의미한 변수일 것\r\n",
    "* ### 가설 2: 문서 사용 트래픽 및 일주일 간 방문일수 역시 결제 행동와 관련 있는 예측 변수일 것"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Expected Output**\r\n",
    "* ### 타깃팅 유저 정보 테이블 (RDB, 일별 배치)\r\n",
    "    * ### Schema: 날짜, 타깃팅 대상 유저의 아이디, 결제 확률, 결제 예상 여부 (결제 확률 50% 이상일 경우 True, 이하일 경우 False)\r\n",
    "* ### 타깃팅 결과 테이블 (RDB, 일별 배치)\r\n",
    "    * ### Schema: 날짜, 타깃 유저 아이디, 타깃 액티비티, 결제 여부(기존 3일내)\r\n",
    "* ### 결제 정보 테이블 (기존 Hadoop 저장 테이블)\r\n",
    "    * ### Schema: 날짜, 전체 결제 유저 아이디, 결제일\r\n",
    "* ### 모델 구축 과정 문서(수식) 및 코드\r\n",
    "* ### 모델 성과 측정 대시보드/리포트 (Recall, Precision, AUC, F1-score)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **04-2. 문제 정의 및 가설**\r\n",
    "--------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 우선 일반적인 모델링 구축을 위한 프로세스를 먼저 살펴보자."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\29.png \"29_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 이번 프로젝트 역시 일반적인 프로세스와 거의 유사하게 진행될 것이다. 리포트 및 공유 문서에 위와 같은 프레임 이미지를 협업자에게 보여준다면 모델링 프로세스에 대한 이해가 높아질 것으로 기대할 수 있다. 위 이미지를 참고하여 이번 프로젝트의 프레임을 구성해보자.\r\n",
    "\r\n",
    "* ### **데이터 수집**\r\n",
    "    * ### 결제 바로 전, 유저의 행동 패턴을 기술할 수 있는 로그 항목을 수집 (문서 오픈, 편집 등)\r\n",
    "* ### **데이터 추출**\r\n",
    "    * ### Extraction, Preprocessing (SQL 필터, 조인 등)\r\n",
    "* ### **데이터 전처리**\r\n",
    "    * ### Feature Engineering (분포 변환, PCA, 결측치 및 이상치 처리 등)\r\n",
    "* ### **모델 구축 및 파라메터 설정**\r\n",
    "    * ### Classification Models (Logistic Regression, Random Forest, etc)\r\n",
    "    * ### Cross Validation, Grid Search, Pipeline\r\n",
    "* ### **모델 평가**\r\n",
    "    * ### Precision, Recall, F1-score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Logic Tree**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 지난 시간에 배운 로직 트리를 이용해 전체적인 흐름을 구성하도록 하자. 모델을 구축하고 평가하는 분석가의 role은 일반적으로 큰 프로젝트 안에서 하나의 부분(part)를 맡게 된다. 비즈니스 환경에서는 단순히 모델을 구축하는 것이 목적이 되기 보다, 별도의 목표를 달성하기 위한 하나의 수단에 지나지 않는 경우가 많다.\r\n",
    "\r\n",
    "### 따라서 전체 프로젝트(숲)을 보고 상황에 맞게 도구(나무)를 활용하려는 노력이 필요하다. 로직트리를 구성할 때는 전체적인 프로젝트를 고려하면서 짜는 것이 유용할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\30.png \"30_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[실습파일](https://github.com/songhunhwa/songhunhwa.github.com/tree/master/tutorial/tutorial_03 \"로직 트리 작성\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **추가 고려 사항**\r\n",
    "### 대부분의 분석 프로젝트도 마찬가지지만, 예측 모델링을 활용하는 프로젝트의 경우 특히 타팀과 팀원간 많은 협업이 요구된다. 분석가가 관여하는 부분은 데이터를 입수해 모델을 구축하고 평가 과정을 통해 최종적인 모델을 만드는 과정이며, 분석가가 만든 프로토타입 코드를 엔지니이와 개발자가 제품에 녹이는 과정도 요구된다. 또 기획 및 운영 등 여러 팀이 관여하게 되므로 전체적인 PM의 역할이 특히 더 중요하다고 할 수 있다.\r\n",
    "\r\n",
    "* ### 전체적인 그리고 세부적인 일정이 어떻게 되는가? 모델링과 관련된 일정을 맞출 수 있는가?\r\n",
    "* ### 각 팀 및 담당자의 역할은 정확히 무엇인가?\r\n",
    "* ### 엔지니어 및 개발 담당은 누구이며, 어떠한 사항이 논의되어야 하는가? 분석 및 개발 언어는? 제품 내 모델 및 데이터 처리 방식은?\r\n",
    "* ### 정확한 목적은 무엇이며 목표 달성 수준은 어떻게 정의할 것인가?\r\n",
    "* ### 필요한 데이터가 무엇인가? 확보가 가능한가?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **04-3. 데이터 전처리**\r\n",
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 모든 데이터 분석 프로젝트에서 데이터 전처리는 반드시 거쳐야 하는 과정이다. 대부분의 데이터 분석가가 좋아하지 않는 과정이지만, 분석 결과/인사이트와 모델 성능에 직접적인 영향을 미치는 과정이기 때문에 중요하게 다루어지는 과정이다. 한 설문조사에 의하면, 분석가의 80% 시간을 데이터 수집 및 전처리에 사용한다고 하니, 얼마나 중요한 과정인지 짐작할 수 있다. 물론 지루하고 반복 작업의 연속이기 때문에 시간이 많이 들어가는 측면도 있을 것이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\31.png \"31_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 지난 시간에 간단히 언급한 대로, 실무에 사용되는 데이터셋은 바로 분석이 불가능할 정도로 지저분(messy)하다. 분석이 가능한 상태로 만들기 위해 아래와 같은 전처리 방식이 자주 사용된다. 모든 강의에 걸쳐서 전처리 단계는 중요하게 그리고 반복적으로 다루어질 예정이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **데이터 불러오기 및 인덱스 지정**\r\n",
    "### 데이터를 읽어 오고 가장 먼저 할일은 첫 행 몇개와 마지막 행을 살펴보는 것이다. 그리고 유니크 식별값을 인덱스로 지정하고 dataframe 각 컬럼의 타입과 결측치 등을 파악하는 것이다.\r\n",
    "```\r\n",
    "# 데이터프레임 읽고 초반, 후반 행 확인하기\r\n",
    "df = pd.read_csv(\"testset.csv\", index_col=0)\r\n",
    "df.head()\r\n",
    "df.tail()\r\n",
    "\r\n",
    "# 인덱스 지정\r\n",
    "df.set_index(\"iduser\", inplace=True)\r\n",
    "\r\n",
    "# 컬럼별 type 확인 및 결측치 확인\r\n",
    "df.info()\r\n",
    "df.isnull().sum()\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **결측치 처리**\r\n",
    "\r\n",
    "### 결측치 처리는 1) 결측치 사례 제거 2) 수치형의 경우 평균이나 중앙치로 대체(imputation)하거나 범주형인 경우 mode 값으로 대체 3) 간단한 예측 모델로 대체하는 방식이 일반적으로 이용된다. 가장 쉬운 방법은 Null이 포함 행 혹은 일부 행을 제거하는 것이다. 수집된 사례(observation)이 많다면 이 방법을 사용하는 것이 가능하다. 만약 샘플수가 충분하지 않을 경우, Pandas의 fillna() 명령어로 Null 값을 채우는 것이 가능하다. 연속형인 경우 Mean이나 Median을 이용하고 명목형인 경우 Mode(최빈치)나 예측 모형을 통해 Null 값을 대체할 수 있다.\r\n",
    "\r\n",
    "### 데이터셋을 읽었다면, Missing Value 파악을 위해 df.info() 가장 처음에 이용하는 것을 추천한다. 만약 np.nan으로 적절히 missing value로 불러왔다면 info() 이용 가능하다. 만약 '', ' ' 이런식의 공백이나 다른 방식으로 처리되어 있다면, 모두 repalce 처리해줘야 한다. info()를 실행했을 때, 누가봐도 float or int 인데 object(string)으로 되어 있다면 이런 사레가 포함될 가능성이 높다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **결측치를 처리할 때 고려할 점**\r\n",
    "\r\n",
    "### 결측치를 처리할 경우에도 도메인 지식은 유용하게 사용된다. 인적, 기계적 원인임이 판명되면, 협업자와 지속적으로 노력해 결측치를 사전에 발생하지 않도록 조치하는 것이 좋다. 수치형인 경우 의미상으로 0으로 메꾸는 것이 맞는지 아니면 평균이나 중앙치가 맞는지 등은 데이터에 대한 배경지식이 있는 경우, 보다 적절한 의사결정을 할 수 있다.\r\n",
    "\r\n",
    "### 예를 들어 viewCount가 1이상인데, edit, export가 missing인 경우 (도메인 지식을 통해) 0으로 메꾸는 것이 가능하다. View 가 다른 행동에 선행하는 개념이기 때문에 위와 같은 의사결정이 가능하다.\r\n",
    "\r\n",
    "* ### NA 와 Null 차이점 (R에서만 구분되는 개념, 파이썬에서는 numpy의 NaN만 이용, 가끔 pure python에서 None을 볼 수 있음, None = empty)\r\n",
    "    * ###  NA: Not Available (does not exist, missing)\r\n",
    "    * ###  Null: empty(null) object\r\n",
    "    * ### NaN: Not a Number (python)\r\n",
    "    * ### reference: https://www.r-bloggers.com/r-na-vs-null/\r\n",
    "\r\n",
    "### 특히 숫자 0과 null 과 같은 결측치는 완전히 다른 개념이니 유의해야 한다. 만약 target(group)에 결측치가 있다면 imputation이 아닌 drop 으로 처리하도록 한다.\r\n",
    "* ### 0: -1과 1 사이의 가운데 숫자(정수)\r\n",
    "* ### null: 미지의 값\r\n",
    "\r\n",
    "```\r\n",
    "# 결측치 부분을 메꾸는 방법\r\n",
    "test['viewCount'] = test['viewCount'].fillna(test.viewCount.mean())\r\n",
    "\r\n",
    "# 만약 결측치가 문자열 스페이스(' ')로 되어 있다면, np.nan으로 바꾸어 Pandas 라이브러리가 인식할수 있도록 변환\r\n",
    "test.viewCount = test.viewCount.replace('', np.nan)\r\n",
    "\r\n",
    "# 결측치를 제거하는 방법\r\n",
    "test.dropna(how='all').head() # 한 행이 모두 missing value이면 제거\r\n",
    "test.dropna(how='any').head() # 한 행에서 하나라도 missing value가 있으면 제거\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **이상치 처리**\r\n",
    "### 일반적으로 1) 표준점수로 변환 후 -3 이하 및 +3 제거 2) IQR 방식 3) 도메인 지식 이용하거나 Binning 처리하는 방식이 이용된다. 표준점수 이용할 경우 평균이 0, 표준편차가 1인 분포로 변환한후 +3 이상이거나 -3 이하인 경우 극단치로 처리한다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\32.png \"32_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "# 표준점수 기반 예제 코드\r\n",
    "def std_based_outlier(df):\r\n",
    "    for i in range(0, len(df.iloc[1])): \r\n",
    "        df.iloc[:,i] = df.iloc[:,i].replace(0, np.NaN) # optional\r\n",
    "        df = df[~(np.abs(df.iloc[:,i] - df.iloc[:,i].mean()) > (3*df.iloc[:,i].std()))].fillna(0)\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IQR 방식은 75% percentile + 1.5 * IQR 이상이거나 25 percentile - 1.5 * IQR 이하인 경우 극단치로 처리하는 방식이다. 이해하기 쉽고 적용하기 쉬운 편이지만, 경우에 따라 너무 많은 사례들이 극단치로 고려되는 경우가 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\33.png \"33_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "# IQR 기반 예제 코드\r\n",
    "def outliers_iqr(ys):\r\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\r\n",
    "    iqr = quartile_3 - quartile_1\r\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\r\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\r\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **데이터 분포 변환**\r\n",
    "### 대부분의 모델은 변수가 특정 분포를 따른다는 가정을 기반으로 한다. 예를 들어 선형 모델의 경우, 설명 및 종속변수 모두가 정규분포와 유사할 경우 성능이 높아지는 것으로 알려져 있다. 자주 쓰이는 방법은 Log, Exp, Sqrt 등 함수를 이용해 데이터 분포를 변환하는 것이다. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "import math\r\n",
    "from sklearn import preprocessing\r\n",
    "\r\n",
    "# 특정 변수에만 함수 적용\r\n",
    "df['X_log'] = preprocessing.scale(np.log(df['X']+1)) # 로그\r\n",
    "df['X_sqrt'] = preprocessing.scale(np.sqrt(df['X']+1)) # 제곱근\r\n",
    "\r\n",
    "# 데이터 프레임 전체에 함수 적용 (단, 숫자형 변수만 있어야 함)\r\n",
    "df_log = df.apply(lambda x: np.log(x+1))  \r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 위 방법 외에도 분포의 특성에 따라 제곱, 자연로그, 지수 등 다양한 함수가 사용될 수 있다. 가이드는 아래와 같다.\r\n",
    "\r\n",
    "* ### left_distribution: X^3\r\n",
    "* ### mild_left: X^2\r\n",
    "* ### mild_right: sqrt(X)\r\n",
    "* ### right: ln(X)\r\n",
    "* ### servere right: 1/X"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **데이터 단위 변환**\r\n",
    "### 데이터의 스케일(측정단위)이 다를 경우 특히 거리를 기반으로 분류하는 모델(KNN 등)에 부정적인 영향을 미치므로, 스케일링을 통해 단위를 일정하게 맞추는 작업을 진행해야 한다. 아래 방식이 주로 스케일링을 위해 쓰이는 방법이다. 대부분의 통계 분석 방법이 정규성 가정을 기반으로 하므로 완벽하지 않더라도 최대한 정규분포로 변환하는 노력이 필요하다.\r\n",
    "\r\n",
    "* ### Scaling: 평균이 0, 분산이 1인 분포로 변환\r\n",
    "* ### MinMax Scaling: 특정 범위 (예, 0~1)로 모든 데이터를 변환\r\n",
    "* ### Box-Cox: 여러 k 값중 가장 작은 SSE 선택\r\n",
    "* ### Robust_scale: median, interquartile range 사용(outlier 영향 최소화)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "from scipy.stats import boxcox\r\n",
    "\r\n",
    "# 변수별 scaling 적용\r\n",
    "df['X_scale'] = preprocessing.scale(df['X']) \r\n",
    "df['X_minmax_scale'] = preprocessing.MinMaxScaler(df['X']\r\n",
    "df['X_boxcox'] = preprocessing.scale(boxcox(df['X']+1)[0])\r\n",
    "df['X_robust_scale'] = preprocessing.robust_scale(df['X'])\r\n",
    "\r\n",
    "# 데이터 프레임 전체에 scaling 적용\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "\r\n",
    "# StandardScaler\r\n",
    "for c in df:\r\n",
    "    df_sc[c] = StandardScaler().fit_transform(df[c].reshape(-1,1)).round(4)\r\n",
    "\r\n",
    "# MinMaxScaler\r\n",
    "for c in df:\r\n",
    "    df_minmax[c] = MinMaxScaler().fit_transform(df[c].reshape(-1,1).round(4))\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[실습파일](https://github.com/songhunhwa/songhunhwa.github.com/tree/master/tutorial/tutorial_03 \"데이터 전처리\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **04-4. 모델 생성**\r\n",
    "------\r\n",
    "\r\n",
    "## **모델 구축 프로세스**\r\n",
    "### 데이터 전처리가 끝났다면, 아래와 같은 프로세스로 모델을 구축한다. 모델링 단계에 들어왔어도 데이터 전처리는 끝난 것은 아니다. 모델의 성능은 알고리즘의 차이 보다 전처리를 어떻게 했는지에 따라 더 많이 영향을 받는 것으로 알려져 있다. 따라서 모델의 성능을 측정해보고 결과가 만족스럽지 않다면, (여러 알고리즘을 사용해보는 것과 더불어) 전처리 단계를 다시 진행해야 할 수도 있다. 또 전처리를 잘하기 위해선 EDA/시각화 과정을 반복해서 진행할 필요가 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\34.png \"34_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **모델 구축전 확인사항**\r\n",
    "### 실제 모델 구축에 들어가기 전에 아래와 같은 사항에 대해 확인하고 진행하는 것을 권장한다.\r\n",
    "\r\n",
    "* ### 범주형, 연속형 데이터 형식의 적절성\r\n",
    "* ### 이상치 및 결측치 처리 여부\r\n",
    "* ### 스케일링 및 분포 변환\r\n",
    "* ### 다중공선성 문제\r\n",
    "* ### 예측변수(y)의 분포 imbalance 문제\r\n",
    "* ### 변수 축소 및 파생 변수 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **분류 모델**\r\n",
    "* ### Logistic Regression\r\n",
    "* ### Naive Bayes\r\n",
    "* ### k-Nearest Neighbor\r\n",
    "* ### Tree-based Model\r\n",
    "    * ### Decision Tree\r\n",
    "    * ### Random Forest\r\n",
    "    * ### Gradient Boost Regression Tree\r\n",
    "* ### SVM (Support Vector Machines)\r\n",
    "* ### ANN (Artificial Neural Network\r\n",
    "    * ### CNN\r\n",
    "    * ### RNN\r\n",
    "## **회귀 모델**\r\n",
    "* ### Linear Regression\r\n",
    "    * ### LASSO\r\n",
    "    * ### Ridge\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **고려사항**\r\n",
    "### 모델 구축을 계획하느 단계부터 아래와 같은 사항을 미리 고려해놓을 필요가 있다.\r\n",
    "\r\n",
    "* ### 목적과 데이터 특성에 맞는 모델은 무엇인가?\r\n",
    "* ### 일반화 가능성은 어떠한가? (overfitting, underfitting의 가능성은?)\r\n",
    "* ### 성능 측정의 지표는? 성능을 높이기 위해 어떻게 Feature Engineering 을 진행할 것인가?\r\n",
    "* ### 제품 혹은 시스템에 모델을 적용할시 계산량이나 언어 특성에 관해 고려할 부분은 무엇인가?\r\n",
    "* ### 모델/파라메터 업데이트 주기 및 방식은 어떻게 협의할 것인가?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Cross Validation**\r\n",
    "### 구축된 모델의 일반화 가능성, 즉 overfitting, underfitting을 다루는 문제는 매우 중요하다. 이를 효과적으로 다룰 수 있는 방법이 Cross Vaildation이다. Cross Validation은 아래와 같은 순서로 진행된다. Testset은 최적화된 파라메터로 구축된 최종 모델의 성능을 파악하기 위해 단 1회만 사용한다. Dataset을 나눌 때 test_size 옵션으로 Train, Test의 비율을 설정할 수 있고, random_state로 seed 값을 지정할 수 있다.\r\n",
    "* ### 전체 데이터셋을 3파트로 분류\r\n",
    "    * ### 50%: Train set\r\n",
    "    * ### 20%: Validation set (for grid search)\r\n",
    "    * ### 30% Test set (to be used just once at the last moment)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\35.png \"35_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "from sklearn.cross_validation import train_test_split\r\n",
    "\r\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=23)\r\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=11)\r\n",
    "```\r\n",
    "### 주로 이용되는 샘플링 방식은 아래와 같다. - Random Sampling - Stratified Sampling (Scikit-learn의 기본 옵션)\r\n",
    "\r\n",
    "```\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Grid Search**\r\n",
    "### 모델의 최적 파라메터를 도출하기 위해서 Kfold 와 Grid Search를 이용한다. Scikit-learn의 GridSearchCV 클래스를 이용하면 쉽게 진행할 수 있다. 해당 모델의 주요 파라메터는 아래와 같다.\r\n",
    "* ### 참고문서: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\r\n",
    "* ### 주요 파라메터 (C)\r\n",
    "    * ### C 값 (기본값 = 1)\r\n",
    "    * ### C 값이 작으면 Penalty 강해짐 (단순 모델)\r\n",
    "    * ### C 값이 크면 Penalty 약해짐 (정규화 없어짐)\r\n",
    "    * ### 보통 로그스케일로 지정(10배씩) = 0.01, 0.1, 1, 10\r\n",
    "* ### penalty\r\n",
    "    * ### L2: Ridge, 일반적으로 사용 (default)\r\n",
    "    * ### L1: LASSO, 변수가 많아서 줄여야할 때 사용, 모델의 단순화 및 해석에 용이"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\36.png \"36_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "# 파라메터 후보\r\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\r\n",
    "              'penalty': ['l1', 'l2']}\r\n",
    "\r\n",
    "# 그리드 서치 진행\r\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)        \r\n",
    "\r\n",
    "# 최종 모델 성능 점검\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "grid_search.score(X_test, y_test)\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 위와 동일한 코드는 아래와 같다. 원리 이해를 위해 코드 작성 및 실행을 해보는 것이 좋다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "# SET default\r\n",
    "best_score = 0\r\n",
    "\r\n",
    "# iterataion\r\n",
    "for r in ['l1', 'l2']:\r\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\r\n",
    "        lm = LogisticRegression(penalty = r, C=C)\r\n",
    "        scores = cross_val_score(lm, X_trainval, y_trainval, cv=5)\r\n",
    "        score = np.mean(scores)\r\n",
    "        if score > best_score:\r\n",
    "            best_score = score\r\n",
    "            best_parameters = {'C': C, 'penalty': r}\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[실습파일](https://github.com/songhunhwa/songhunhwa.github.com/tree/master/tutorial/tutorial_03 \"모델 생성 및 학습\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **04-5. 모델 평가**\r\n",
    "----\r\n",
    "## **평가 Matric**\r\n",
    "### 회귀 및 분류 문제에 따라 평가하는 지표가 차이가 있다. 이번 프로젝트의 경우 이진 분류 문제에 해당하므로 Confusion Matrix 와 ROC Curve를 통해 모델을 평가하고 개선하는 작업을 진행하고자 한다. 참고로 평가지표를 설정할 경우에는 상황/환경, 우선순위 등에 따라 협의하에 선정된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* ### 회귀 문제"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\37.png \"37_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* ### 분류 문제"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\38.png \"38_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scikit-learn에서는 분류 문제와 관련한 성과 지표를 쉽게 확인할 수 있도록 아래 클래스를 제공한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "from sklearn.metrics import confusion_matrix, classification_report\r\n",
    "\r\n",
    "print(confusion_matrix(grid_search.predict(X_test), y_test))\r\n",
    "print(classification_report(grid_search.predict(X_test), y_test))\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\39.png \"39_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\r\n",
    "from sklearn.metrics import roc_curve, auc\r\n",
    "\r\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, grid_search.predict(X_test))\r\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(6,4))\r\n",
    "plt.title('Receiver Operating Characteristic')\r\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\r\n",
    "plt.legend(loc='lower right')\r\n",
    "plt.plot([0,1],[0,1],'r--')\r\n",
    "plt.xlim([-0.1,1.2])\r\n",
    "plt.ylim([-0.1,1.2])\r\n",
    "plt.ylabel('True Positive Rate')\r\n",
    "plt.xlabel('False Positive Rate')\r\n",
    "```\r\n",
    "\r\n",
    "[실습파일](https://github.com/songhunhwa/songhunhwa.github.com/tree/master/tutorial/tutorial_03 \"모델 평가\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **04-6. 모델 성과 개선**\r\n",
    "----\r\n",
    "\r\n",
    "### 첫 번째 모델의 성능을 baseline 으로 참고하여 전처리를 다시하거나 변수 선택 및 여러 모델 등을 적용해보며 퍼포먼스를 올리는 과정이 필요하다.\r\n",
    "\r\n",
    "## **변수 스케일링 및 분포 변환**\r\n",
    "### 모델의 성능을 높이기 위해 변수의 분포를 변환시키나 스케일을 조정하는 등 전처리 과정을 반복적으로 진행할 필요가 있다. 모델 특성 및 데이터 특성 등에 따라 적합한 전처리 과정이 다르므로 여러 번의 시도를 통해 모델의 성능을 올릴 필요가 있다.\r\n",
    "* ### Standardization, Mean Normalizatoin, MinMax Scaling\r\n",
    "* ### Log, Sqrt, Sqaure for distribution transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Upsampling & Downsampling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 타깃 변수의 그룹별 비율이 차이가 나는 경우, (예 결제자의 수가 매우 적고 비결제자의 수가 많은 경우) Imbalance 분포를 보이게 된다. 이와 같은 경우 모델이 학습을 위해 요구하는 충분한 데이터가 제공되지 않으므로 아래와 같은 방법을 고려해야 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\40.png \"40_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* ### Collect More Data (if possible)\r\n",
    "* ### Resampling the Dataset\r\n",
    "    * ### **Oversampling** (no inofromation loss, perform better than undersampling, but overfitting issues)\r\n",
    "    * ### **Undersampling** (help improve run time and storage problems, but information loss, biased dataset)\r\n",
    "* ### Generate Synthetic Samples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Feature Selection**\r\n",
    "### 모델에 적용할 input 변수를 잘 선정하는 것은 모델의 성능에 직접적인 영향을 미치는 과정이다. 아래의 방법을 이용해 모델의 성능을 높이는 데 유용한 변수를 선택할 필요가 있다.\r\n",
    "* ### Univariate Selection (T-test, ANOVA, Coefficient and so on)\r\n",
    "* ### Feature Importance (from Tree-based model)\r\n",
    "* ### RFE (recursive feature elimination)\r\n",
    "### 우선 Univariate Selection은 그룹내 분산이 작고 그룹간 분산이 클 경우 값이 커지는 F-value를 이용하여 변수를 선택한다. 각 변수마다 F값을 구해 F값이 큰 변수를 기준으로 변수를 선택하는 방법이다.\r\n",
    "```\r\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\r\n",
    "\r\n",
    "selectK = SelectKBest(score_func=f_classif, k=8)\r\n",
    "X = selectK.fit_transform(X, y)\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ExtraTreesClassifier와 같은 트리 기반 모델은 Feature Importance 를 제공한다. 이 Feature Importance는 불확실도를 많이 낮출수록 증가하므로 이를 기준으로 변수를 선택할 수 있다.\r\n",
    "```\r\n",
    "from sklearn.ensemble import ExtraTreesClassifier\r\n",
    "\r\n",
    "etc_model = ExtraTreesClassifier()\r\n",
    "etc_model.fit(X, y)\r\n",
    "\r\n",
    "print(etc_model.feature_importances_)\r\n",
    "feature_list = pd.concat([pd.Series(X.columns), pd.Series(etc_model.feature_importances_)], axis=1)\r\n",
    "feature_list.columns = ['features_name', 'importance']\r\n",
    "feature_list.sort_values(\"importance\", ascending =False)[:8]\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 마지막으로 RFE (recursive feature elimination)는 Backward 방식중 하나로, 모든 변수를 우선 다 포함시킨 후 반복해서 학습을 진행하면서 중요도가 낮은 변수를 하나씩 제거하는 방식이다.\r\n",
    "```\r\n",
    "from sklearn.feature_selection import RFE\r\n",
    "\r\n",
    "model = LogisticRegression()\r\n",
    "rfe = RFE(model, 8)\r\n",
    "fit = rfe.fit(X, y)\r\n",
    "\r\n",
    "print(\"Num Features: %d\") % fit.n_features_\r\n",
    "print(\"Selected Features: %s\") % fit.support_\r\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Model Selection**\r\n",
    "### Scikit-learn은 전처리(스케일랑, feature selection, model selection)과 grid search 를 한번에 진행할 수 있도록 파이프라인 기능을 제공한다.\r\n",
    "```\r\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "pipe = Pipeline([('classifier', LogisticRegression())])\r\n",
    "\r\n",
    "param_grid = [{'classifier': [SVC()], \r\n",
    "              'classifier__gamma': [0.01, 0.1, 1, 10, 100], \r\n",
    "              'classifier__C': [0.01, 0.1, 1, 10, 100]\r\n",
    "              },\r\n",
    "\r\n",
    "               {'classifier': [LogisticRegression()],\r\n",
    "               'classifier__penalty': ['l1', 'l2'], \r\n",
    "               'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\r\n",
    "               },\r\n",
    "\r\n",
    "              {'classifier': [RandomForestClassifier()],\r\n",
    "              'classifier__max_depth': [4, 6], # max_depth: The maximum depth of the tree.\r\n",
    "              'classifier__n_estimators': [50, 100], # n_estimators: The number of trees in the forest.\r\n",
    "              'classifier__min_samples_split': [50, 100]\r\n",
    "              }] # min_samples_split: The minimum number of samples required to split an internal node       \r\n",
    "\r\n",
    "grid = GridSearchCV(pipe, param_grid, scoring = 'roc_auc', cv=5)  \r\n",
    "grid.fit(X_train, y_train)\r\n",
    "\r\n",
    "print(grid.best_params_)\r\n",
    "print(grid.best_score_)\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[실습파일](https://github.com/songhunhwa/songhunhwa.github.com/tree/master/tutorial/tutorial_03 \"모델 성과 개선\")"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('AIMath': conda)"
  },
  "interpreter": {
   "hash": "53a6f66994d44a531c15dda9534cca6a0a046d02bce3524de6c1f8e9a8c07fb2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}