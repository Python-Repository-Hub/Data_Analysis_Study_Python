{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **02.분석 실무에 대한 이해 Part 2**\r\n",
    "-------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **목적**\r\n",
    "* ### 분석가를 둘러싼 시스템적 환경 및 툴, 필수 개념에 대해 학습한다.\r\n",
    "* ### 주요 데이터 형태인 로그 데이터를 설계하는 방법에 대해 이해한다.\r\n",
    "* ### JSON 형태의 로그 데이터를 파싱하는 작업과 SQL를 통해 추출 및 간단한 전처리 실습을 진행한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **목차**\r\n",
    "* ### Apache Spark & Modules 소개\r\n",
    "* ### AWS 소개\r\n",
    "* ### 클라이언트 로그 설계 사례\r\n",
    "* ### Json 및 Text 형태의 로그 데이터 Parsing 실습\r\n",
    "* ### SQL 및 Pandas를 통한 추출 및 전처리 실습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **02-1. Spark & 주요 Modules 소개**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **데이터 분석 환경**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 분석 환경은 주로 엔지니어 및 회사 고유의 상황에 따라 결정된다. 분석가는 환경적/구조적 특성과 제한점 등 여러 사항을 고려하여 분석을 진행한다. 특히 데이터 수집 과정을 분석 목적에 맞게 최적화 하는 등의 목적을 위해 분석가가 환경 및 구조에 관여하기도 한다. 물론, 분석가가 주도적으로 처음부터 환경을 설정하고 구조를 쌓아올라가는 경우도 있지만 이는 일반적인 상황이라고 보기 어렵다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 분석가가 좋은 성과를 내기 위해서는 분석 환경을 잘 이해/활용하고 때로는 (분석 관점에 맞게) 개선점을 엔지니어에게 전달하는 등 역할이 필요하다. 따라서 (실무는 엔지니어가 진행하더라도) 환경/시스템적 요소에 대한 이해와 지속적인 관여 역시 분석가의 역할이기도 하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **스파크 소개**\r\n",
    "### 최근 비정형 데이터의 생성과 매우 큰 사이즈 등의 이슈로 기존 RDBS에서 하둡/스파크를 도입하는 추세이다. 비록 RDBS만큼 즉각적 생성/수정/변경 등은 어렵지만, Spark나 하둡을 이용할 경우 분산 저장 및 처리를 통해 빠른 분석 진행이 가능하다. 최근에는 하둡 보다 분석 친화적인 스파크를 주로 사용해 분석하는 추세이다. 스파크가 Pyspark이나 SparkR 같은 다양한 분석 API를 제공하고 있기 때문이다. 참고로 하둡은 Java, Spark는 원래 스칼라 기반이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\11.png \"11_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 스파크에서 다루는 주요 데이터 타입은 RDD(Resilient Distributed Datasets)와 DataFrame이다. 기존 하둡에서는 디스크에서 데이터 I/O가 발생하는 반면, 스파크는 RAM에서 발생하게 설정할 수 있으므로 속도에서 비약적인 차이가 발생한다. 최근에는 RDD보다 DataFrame을 이용하는 추세이며(RDBS의 테이블이나 Pandas Dataframe과 유사하기 때문), Spark의 특징인 Lazy execution을 통해 보다 효율적인 처리/분석이 가능하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lazy Execution은 함수를 Transform, Action 으로 구분해 Action 일 경우에만 실제 실행이 발생하는 것을 의미한다. 매번 결과를 갖고 오지 않고, 필요한 경우에만 RAM을 통해 데이터 I/O가 발생하므로 분석 작업 속도가 매우 높아진다. Spark에서 데이터 분석을 하는 경우, 매우 큰 사이즈의 데이터를 다루는 경우가 많아 이러한 매커니즘은 매우 중요한 장점으로 작용한다. (다행히 Transform 단계라도 에러를 내보내므로 Action 단계에서 제대로 결과가 나왔는지 걱정할 필요는 없다)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **RDD**\r\n",
    "* ### Distribute collection of JVM objects\r\n",
    "* ### Funtional Operators (map, filter, reduceByKey, ect)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\12.png \"12_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **DataFrame**\r\n",
    "* ### Distribute collection of Row objects\r\n",
    "* ### Expression-based operations and UDFs\r\n",
    "* ### ogical plans and optimizer\r\n",
    "* ### Fast/efficient internal reprenstations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\13.png \"13_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Lazy Execution**\r\n",
    "* ### Transfrom: filter, select, drop, join, dropDuplicates, distinct, withColumn, pivot, get_json_object, sample\r\n",
    "* ### Action: count, collect, show, head, take\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\14.png \"14_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Spark 데이터 추출 및 전처리**\r\n",
    "* ### Download Page\r\n",
    "* ### Github Page for Tutorial\r\n",
    "* ### SparkContext 생성\r\n",
    "* ### DataFrame 생성 및 추출\r\n",
    "* ### 전처리 및 분석"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\r\n",
    "# import modules\r\n",
    "from pyspark.sql import SQLContext\r\n",
    "from pyspark.sql.functions import *\r\n",
    "\r\n",
    "# read the csv with library\r\n",
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\r\n",
    "                    .options(header='true', inferSchema='true')\\\r\n",
    "                    .load('/Users/woowahan/Documents/Python/DS_Ext_School/tutorial_01/doc_use_log.csv')\r\n",
    "\r\n",
    "# convert the df to tmp table (as if it's in database)\r\n",
    "df.registerTempTable(\"df_tmp\")\r\n",
    "\r\n",
    "# extract data from table with sql\r\n",
    "df1 = sqlContext.sql(\"select ismydoc, actiontype, sessionid, datetime from df_tmp where ismydoc = true\")\r\n",
    "\r\n",
    "## Lazy Execution\r\n",
    "df2 = sqlContext.sql(\"select * from df_tmp\")\r\n",
    "\r\n",
    "df2_pdf = df2.select(\"sessionid\", \"ext\").filter(\" ext == 'PDF' or ext = 'DOC'\").dropDuplicates().cache()\r\n",
    "df2.distinct().count()\r\n",
    "\r\n",
    "df2_min_date = df2.groupby(\"sessionid\").agg(min(\"datetime\").alias(\"min_date\"))\r\n",
    "df2_min_date.show()\r\n",
    "\r\n",
    "df2_join = df2_pdf.join(df2_min_date, \"sessionid\", \"left\")\r\n",
    "df2_join.show()\r\n",
    "\r\n",
    "df2_join1 = df2_join.groupby(\"min_date\", \"ext\").agg(count(\"sessionid\").alias(\"cnt\"))\r\n",
    "\r\n",
    "df2_join1.describe().show()\r\n",
    "\r\n",
    "# Pandas\r\n",
    "df2_pd = df2.toPandas()\r\n",
    "df2_pd.head()\r\n",
    "df2_pd.describe()```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **스파크 Modules & 머신러닝**\r\n",
    "### 스파크가 최근에 각광을 받게 된 배경에는 스파크가 제공하는 모듈도 영향을 미쳤다. 스파크는 분산처리프레임 위에 Spark Streaming, SparkSQL, MLlib, GraphX와 같은 모듈을 제공하여 실시간 수집부터 데이터 추출/전처리, 머신러닝 및 그래프 분석까지 하나의 흐름에 가능하도록 개발되었다. 각 모듈의 특성을 살펴보자."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* ### Spark SQL: Spark Wrapper 함수에 SQL 쿼리를 넣어 추출/전처리/분석이 쉽게 가능하도록 지원\r\n",
    "* ### MLlib: 머신러닝 알고리즘 제공 (코드 예시)\r\n",
    "* ### Spark Streaming: 실시간 데이터 처리\r\n",
    "* ### GraphX: 그래프 분석 라이브러리\r\n",
    "### 위 4개의 모듈 중에 분석가가 많이 사용하는 것은 Spark SQL과 Mllib이다. 아래 예시 코드를 보자."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\r\n",
    "# import modules\r\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler\r\n",
    "from pyspark.ml import Pipeline\r\n",
    "from pyspark.ml.classification import LogisticRegression\r\n",
    "from pyspark.sql.functions import col, stddev_samp\r\n",
    "\r\n",
    "# read datafiles\r\n",
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\r\n",
    "                    .options(header='true', inferSchema='true')\\\r\n",
    "                    .load('/Users/woowahan/Documents/Python/DS_Ext_School/tutorial_01/Default.csv')\\\r\n",
    "                    .drop(\"_c0\")\\\r\n",
    "                    .cache()\r\n",
    "\r\n",
    "# transform categorical values to int\r\n",
    "strIdx = StringIndexer(inputCol = \"student\", outputCol = \"studentIdx\")\r\n",
    "\r\n",
    "# one-hot encoding\r\n",
    "encode = OneHotEncoder(inputCol = \"studentIdx\", outputCol = \"studentclassVec\")\r\n",
    "\r\n",
    "# transform categorical values to int\r\n",
    "label_StrIdx = StringIndexer(inputCol = \"default\", outputCol = \"label\")\r\n",
    "\r\n",
    "# set stages for pipeline\r\n",
    "stages = [strIdx, encode, label_StrIdx]\r\n",
    "\r\n",
    "# columns\r\n",
    "numCols = ['income', 'balance']\r\n",
    "for c in numCols:\r\n",
    "    df = df.withColumn(c + \"Scaled\", col(c)/df.agg(stddev_samp(c)).first()[0])\r\n",
    "\r\n",
    "# set inputs and append it to the stage\r\n",
    "inputs = [\"studentclassVec\", \"incomeScaled\", \"balanceScaled\"]\r\n",
    "assembler = VectorAssembler(inputCols = inputs, outputCol = \"features\")\r\n",
    "stages += [assembler]\r\n",
    "\r\n",
    "# create pipeline\r\n",
    "pipeline = Pipeline(stages = stages)\r\n",
    "pipelineModel = pipeline.fit(df)\r\n",
    "dataset = pipelineModel.transform(df)\r\n",
    "\r\n",
    "# cross validation and fit models\r\n",
    "(train, test) = dataset.randomSplit([0.7, 0.3], seed = 14)\r\n",
    "lr = LogisticRegression(labelCol = \"label\", featuresCol = \"features\", maxIter=10)\r\n",
    "\r\n",
    "lrModel = lr.fit(train)\r\n",
    "predictions = lrModel.transform(test)\r\n",
    "predictions.show()\r\n",
    "\r\n",
    "# evaluation\r\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\r\n",
    "\r\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\r\n",
    "evaluator.evaluate(predictions) # AUC\r\n",
    "\r\n",
    "# grid search for parametor tuning\r\n",
    "paramGrid = (ParamGridBuilder()\r\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\r\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\r\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\r\n",
    "             .build())\r\n",
    "\r\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\r\n",
    "cvModel = cv.fit(train)\r\n",
    "predictions = cvModel.transform(test)\r\n",
    "evaluator.evaluate(predictions) ```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **02-2. AWS 소개 및 로그 정의/파싱**\r\n",
    "-------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **AWS 소개**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 데이터를 수집하고 저장, 처리 및 분석하는 일련의 과정을 직접 구현하기에 많은 인력과 자원/시간이 소모된다. 이를 쉽게 가능하도록 클라우드 플랫폼 솔루션을 제공하는 것이 AWS(Amazon Web Service)이다. 사용량 비례 과금 방식으로 잘 설계된 저장소와 서버 등 일련의 플랫폼을 저렴하게 사용할 수 있는 것이 가장 큰 장점이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 분석가의 입장에서 가장 긴밀한 서비스는 EMR이다. EMR은 Spark, Hadoop, Presto, HBase 등 분석에 유용한 분산프레임워크를 제공한다. 또 Amazon S3 및 Amazon DynamoDB와 같은 저장소와 호환되므로 데이터 저장/입출력/추출/분석 등이 효율적으로 진행된다. 실무에서의 분석은 로컬에서 이뤄지지 않고 Amazon EMR Cluster를 띄워 사용당 과금을 하며, 서버에서 진행된다. 물론 자체적으로 구축된 서버를 이용하여 분석을 진행하기도 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![screenshot](.\\img\\15.png \"15_Sh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **로그 정의/설계**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 로그 데이터는 최근 사용자의 사용성 및 행동 패턴을 확인하거나 유저 클러스터링, 모델링 등 다양한 목적으로 사용되는 행동 기반 데이터이다. 로그는 설문과 같은 사용자 응답 및 기억에 의존하는 데이터 수집 방법 대비, 행동을 정확하게 파악/예측할 수 있는 장점이 있다. 또 RDB의 결과론적인 데이터와 달리 특정 결과에 이르는 과정과 흐름을 상세히 파악할 수 있어, 서비스를 개선하는 데 매우 유용한 자료이다. 대신 데이터 용량이 크기 때문에 스토리지 관련 비용/리소스가 발생하고, JSON, CSV, TSV와 같은 비정형 텍스트 형태이므로 기존 RDB와는 다른 수집/처리 시스템과 전문 인력이 요구된다는 단점을 가지고 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 분석가의 역할 중에 로그 정의 및 설계가 중요한 역할이다. 분석가는 산재된 로그를 분석 목적에 맞게 포멧을 정리하고 로깅할 항목을 우선순위에 맞게 정하는 역할을 한다. 또 로그 발생시 수집할 필드명과 값의 이름을 정의하고 설계하는 업무를 맡는다. 실제 데이터 수집/처리시 정의한 대로 로그가 쌓이므로, 이 단계는 매우 중요한 단계라고 할 수 있다. 더불어 쌓인 로그의 데이터 퀄리티를 확인하고 관리하는 역할 역시 분석가의 몫이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **로그 정의 예시**\r\n",
    "### 최근 로그의 형태는 대부분 JSON(JavaScript Object Notation)이다. Pandas의 Dictionary와 거의 유사하게 Key, Value로 구성되어 있으며, Hierchial 구조를 가질 수 있다. 분석가는 로그 송출시 Json의 Key와 Value에 들어갈 값을 정한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\r\n",
    "# 로그 스키마 예시\r\n",
    "{\r\n",
    " \"memid\": \" \", # int \r\n",
    " \"sessionid\": \" \", # string \r\n",
    " \"ver\": \" \", # string\r\n",
    " \"screen\": \"Main\", # string\r\n",
    " \"event\": \"View\", # string\r\n",
    " \"area\": \"Seoul\", # string\r\n",
    " \"group\": \"A\", # string, A or B ...\r\n",
    " \"params\": {\r\n",
    "            \"isGuest\": \"T\", # boolean \r\n",
    "            \"UserType\": \" \" # string\r\n",
    "            }\r\n",
    "}\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **JSON Parsing**\r\n",
    "### 로그 정의후 수집이 이루어지면, JSON과 Pandas Library를 통해 Pandas DataFrame 형태로 아래와 같이 파싱할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "data= []\r\n",
    "mydf = pd.DataFrame()\r\n",
    "\r\n",
    "# to parse json files\r\n",
    "with open('파일명.json') as f:\r\n",
    "    for line in f:\r\n",
    "        data.append(json.loads(line))\r\n",
    "\r\n",
    "    # convert json to df\r\n",
    "    for i in range(0, len(data)):\r\n",
    "        df = pd.DataFrame.from_dict([data[i]])\r\n",
    "        mydf = mydf.append(df)\r\n",
    "\r\n",
    "# reset index\r\n",
    "mydf.reset_index(drop=True, inplace=True)\r\n",
    "\r\n",
    "# change column types\r\n",
    "mydf['date'] = pd.to_datetime(mydf['date'], unit='s').dt.date\r\n",
    "mydf['date'] = mydf['date'].astype('datetime64[ns]')\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[실습파일](https://github.com/songhunhwa/songhunhwa.github.com/tree/master/tutorial/tutorial_01 \"JSON Client Log Parsing with JSON Library\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **02-3. SQL 데이터 추출**\r\n",
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **데이터 추출**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 이번에는 이미 수집/처리된 테이블에서 데이터를 불러오고 간단히 전처리는 방법을 알아보도록 한다. SQL 언어는 데이터를 추출하고 간단하게 전처리, 탐색을 할때 매우 유용한 언어이다. 실습을 위해 실제 db를 설치하고 환경을 세팅하는 것이 불가능하므로, 기능이 유사한 Python Library(pandasql)를 활용한다. 라이브러리 설치 방법은 아래와 같다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "``` $ pip install -U pandasql ```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 아래와 같이 라이브러리를 임포트한 후에 쿼리를 작성하고 실행해보도록 하자. db가 없으므로 read_csv로 불러온 csv파일이 db의 테이블이라고 가정할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python \r\n",
    "# db의 테이블이라고 가정할 csv\r\n",
    "df = pd.read_csv(\"/Users/woowahan/Documents/Python/DS_Ext_School/tutorial_01/doc_use_log.csv\")\\\r\n",
    "       .sample(frac=0.01, replace=False) # 빠른 진행을 위해 샘플링\r\n",
    "\r\n",
    "# SQL 언어 활용을 위한 라이브러리, 쿼리 작성\r\n",
    "from pandasql import *\r\n",
    "\r\n",
    "q = \"SELECT * \r\n",
    "        FROM df \r\n",
    "        WHERE ext = 'PDF' \r\n",
    "          AND ismydoc = '0' \r\n",
    "         LIMIT 10\"\r\n",
    "print sqldf(q, locals()).to_string()\r\n",
    "\r\n",
    "# SQL group by 함수를 이용한 집계 처리\r\n",
    "q = \"\"\"\r\n",
    "    SELECT \r\n",
    "      ext, \r\n",
    "      count(ext) as count, \r\n",
    "      count(distinct sessionid) as unq_sess \r\n",
    "    FROM df \r\n",
    "    GROUP BY ext \r\n",
    "    ORDER BY count DESC\r\n",
    "    \"\"\"\r\n",
    "print sqldf(q, locals()).to_string()\r\n",
    "\r\n",
    "# 테이블 조인\r\n",
    "ios = pd.read_csv(\"/Users/woowahan/Documents/Python/DS_Ext_School/tutorial_01/ios.csv\")\r\n",
    "\r\n",
    "q = \"\"\"\r\n",
    "  SELECT \r\n",
    "    A.*, \r\n",
    "    B.flag \r\n",
    "  FROM df A\r\n",
    "  LEFT JOIN\r\n",
    "    (\r\n",
    "    SELECT sessionid, flag\r\n",
    "    FROM ios\r\n",
    "    ) B\r\n",
    "   ON A.sessionid = B.sessionid\r\n",
    "  WHERE B.flag = 'iOS'\r\n",
    "  \"\"\"\r\n",
    "print sqldf(q, locals()).to_string()```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **연습문제**\r\n",
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 위 실습 데이터로 아래 연습문제를 SQL로 풀어보고, 동일한 작업을 Pandas를 통해 통해 진행해보는 것이 좋다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 1**\r\n",
    "* ### df 테이블의 Action Type 값(항목)별 유니크한 세션수는?\r\n",
    "* ### 유니크 세션수 기준으로 내림차순 정렬하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 2**\r\n",
    "* ### ismydoc이 1(True)인 경우에 한해, 날짜별 세션수의 유니크 빈도 구하기\r\n",
    "* ### 유니크 빈도가 가장 큰 top 5 날짜 확인하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 3**\r\n",
    "* ### 문서 포지션별(documentposition)로 자주 OPEN 되는 확장자(ext)를 확인하기\r\n",
    "* ### 카운트 기준: unique sessionid\r\n",
    "* ### 그룹별, 세션카운트 기준 desc 정렬"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[실습파일](https://github.com/songhunhwa/songhunhwa.github.com/tree/master/tutorial/tutorial_01 \"SQL를 이용한 데이터 추출\")"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('AIMath': conda)"
  },
  "interpreter": {
   "hash": "53a6f66994d44a531c15dda9534cca6a0a046d02bce3524de6c1f8e9a8c07fb2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}